

# K8S 总结

## 介绍

- Docker 的一些常用方法，当然我们的重点会在 Kubernetes 上面
- 会用 kubeadm 来搭建一套 Kubernetes 的集群
- 理解 Kubernetes 集群的运行原理
- 常用的一些控制器使用方法
- 还有 Kubernetes 的一些调度策略
- Kubernetes 的运维，使用 Prometheus 来进行集群监控报警，EFK 进行日志收集
- 包管理工具 Helm 的使用
- 基于 Kubernetes 的 CI/CD
- Istio 在 Kubernetes 中的使用

---

## Docker基础

### 简介

#### LXC 介绍

LXC 可以提供轻量级的虚拟化，用来隔离进程和资源，和我们传统观念中的全虚拟化完全不一样，非常轻量级。LXC 可以将单个操作系统管理的资源划分到独立的组中，和传统的虚拟化技术相比，LXC 有如下一些优势：

- 和宿主机使用同一个内核，所以性能损耗小
- 不需要指令级模拟
- 不需要即时编译
- 容器可以在 CPU 核心的本地运行指令，不需要任何专门的解释机制
- 避免了虚拟化和系统调用中的复杂性
- 轻量级隔离，隔离的同时还可以和宿主机共享资源



#### Docker

Docker 是一个开源的应用容器引擎，基于 go 语言开发，可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 服务器。容器是一个沙箱机制，相互之间不会有影响（类似于我们手机上运行的 app），并且容器开销是很低的。

**特点**：

- 灵活性：即使是最复杂的应用也可以集装箱化
- 轻量级：容器利用并共享主机内核
- 可互换：您可以即时部署更新和升级
- 便携式：您可以在本地构建，部署到云，并在任何地方运行
- 可扩展：您可以增加并自动分发容器副本
- 可堆叠：您可以垂直和即时堆叠服务

**概念**：

- **镜像**：一个只读模板，带有创建 Docker 容器的说明，一般来说的，镜像会基于另外的一些基础镜像并加上一些额外的自定义功能来组成。
-  **容器**：是一个镜像的可运行的实例，可以使用 Docker REST API 或者 CLI 命令行工具来操作容器，容器的本质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户 ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安全。
- **仓库**：是用来存储 Docker 镜像的仓库，Docker Hub 是 Docker 官方提供的一个公共仓库，而且 Docker 默认也是从 Docker Hub 上查找镜像的，当然你也可以很方便的运行一个私有仓库，当我们使用 `docker pull` 或者 `docker run` 命令时，就会从我们配置的 Docker 镜像仓库中去拉取镜像，使用 `docker push` 命令时，会将我们构建的镜像推送到对应的镜像仓库中，registry 可以理解为用于镜像的 github 这样的托管服务。

**Namspace**：

- pid namespace：用于进程隔离（PID：进程ID）
- net namespace：管理网络接口（NET：网络）
- ipc namespace：管理对 IPC 资源的访问（IPC：进程间通信（信号量、消息队列和共享内存））
- mnt namespace：管理文件系统挂载点（MNT：挂载）
- uts namespace：隔离主机名和域名
- user namespace：隔离用户和用户组

**CGroups**

- 资源限制：可以对任务使用的资源总额进行限制
- 优先级分配：通过分配的 cpu 时间片数量以及磁盘 IO 带宽大小等，实际上相当于控制了任务运行优先级
- 资源统计：可以统计系统的资源使用量，如 cpu 时长，内存用量等
- 任务控制：cgroup 可以对任务执行挂起、恢复等操作

**Dockerfile**

- Docker 镜像是由一系列的层组成的，每层代表 Dockerfile 中的一条指令

  ```yaml
  FROM ubuntu:18.04
  COPY . /app
  RUN make /app
  CMD python /app/app.py
  ```

**Docker 架构**

![image-20230220145046612](https://raw.githubusercontent.com/MarcLan/pic/main/image-20230220145046612.png)

Docker安装

要安装 Docker CE（社区）版本，推荐使用 CentOS 7，使用 `overlay2` 存储驱动程序。



### 安装

要安装 Docker CE（社区）版本，推荐使用 CentOS 7，使用 `overlay2` 存储驱动程序。

#### 卸载旧版本

旧版本的 Docker 叫 `docker` 或者 `docker-engine`，现在的版本被称为`docker-ce`，如果之前安装过，则先卸载掉之前的版本：

```shell
$ sudo yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine
```

#### 安装 Docker-CE

要安装 Docker 有多种方法，大多数用户会设置 Docker 的存储仓库来进行安装，这种方式可以简化我们的安装和升级，当然也推荐使用这种方式。

但是如果你的服务器无法访问互联网则可以使用手动下载 RPM 安装包进行安装，如果你要使用这种方式安装，可以在页面 https://download.docker.com/linux/centos/7/x86_64/stable/Packages/ 下载自己想要安装的 `.rpm` 文件然后直接安装即可。

我们这里通过在线方式进行安装，首先安装仓库，先安装一些依赖包，其中`yum-utils`提供了`yum-config-manager`的一些实用的功能：

```shell
$ sudo yum install -y yum-utils
```

然后用下面的命令添加 `stable` 版本的 docker 仓库：

```shell
$ sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo
```

添加完成后就可以安装了，如果要安装最新版本，直接执行下面的命令即可：

```shell
$ yum install docker-ce docker-ce-cli containerd.io
```

如果要安装指定的版本，可以通过如下命令来获取可安装的版本：

```shell
$ yum list docker-ce --showduplicates | sort -r
 * updates: mirrors.tuna.tsinghua.edu.cn
Loading mirror speeds from cached hostfile
Loaded plugins: fastestmirror, langpacks
 * extras: mirrors.huaweicloud.com
docker-ce.x86_64            3:19.03.4-3.el7                     docker-ce-stable
......
docker-ce.x86_64            3:18.09.9-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.8-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.7-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.6-3.el7                     docker-ce-stable
......
docker-ce.x86_64            18.06.0.ce-3.el7                    docker-ce-stable
```



该软件包名称是软件包名称（docker-ce）加上版本字符串（第二列），从第一个冒号（:）开始，直至第一个连字符，并用连字符（-）分隔 ）。 比如这里我们来安装 docker-ce-18.09.9 版本：

```shell
$ sudo yum install docker-ce-18.09.9 docker-ce-cli-18.09.9 containerd.io -y
```

安装完成，但是还没有启动，由于我这里的磁盘空间不大，所以需要更改下 Docker 的根目录，将默认的 `/var/lib/docker` 更改为 `/data/docker` 目录，添加如下配置文件：

```shell
$ mkdir -p /etc/docker
$ vi /etc/docker/daemon.json
{
  "registry-mirrors" : [
    "https://ot2k4d59.mirror.aliyuncs.com/"
  ],
  "graph": "/data/docker"
}
```

镜像加速器

使用加速器可以提升获取 Docker 官方镜像的速度，建议注册阿里云帐号，使用阿里云提供的镜像加速服务，地址：https://cr.console.aliyun.com/cn-beijing/instances/mirrors。

然后执行下面的命令启动 docker：

```shell
# 设置为开机启动
$ systemctl enable docker  
$ systemctl daemon-reload
# 启动 docker
$ systemctl start docker
```

启动成功后，可以通过如下命令查看 Docker 相关信息：

```shell
$ docker info
docker info
Containers: 0
 Running: 0
 Paused: 0
 Stopped: 0
Images: 0
Server Version: 18.09.9
Storage Driver: overlay2
 Backing Filesystem: extfs
 Supports d_type: true
 Native Overlay Diff: false
Logging Driver: json-file
Cgroup Driver: cgroupfs
Plugins:
 Volume: local
 Network: bridge host macvlan null overlay
 Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
Swarm: inactive
Runtimes: runc
Default Runtime: runc
Init Binary: docker-init
containerd version: b34a5c8af56e510852c35414db4c1f4fa6172339
runc version: 3e425f80a8c931f88e6d94a8c831b9d5aa481657
init version: fec3683
Security Options:
 seccomp
  Profile: default
Kernel Version: 3.10.0-327.el7.x86_64
Operating System: CentOS Linux 7 (Core)
OSType: linux
Architecture: x86_64
CPUs: 4
Total Memory: 7.64GiB
Name: pnhmltdr
ID: VIBG:JCTE:KBVO:ETXD:OZH4:SXCH:MA7Y:YFFB:KQEX:OLAM:I25P:F2TM
Docker Root Dir: /data/docker
Debug Mode (client): false
Debug Mode (server): false
Registry: https://index.docker.io/v1/
Labels:
Experimental: false
Insecure Registries:
 127.0.0.0/8
Registry Mirrors:
 https://ot2k4d59.mirror.aliyuncs.com/
Live Restore Enabled: false
Product License: Community Engine
```

可以看到安装的版本是 18.09.9，默认的存储驱动程序是 `overlay2`，而且 `Root Dir` 也已经是我们更改后的目录了。

#### 测试

Docker 启动成功后，我们可以使用下面的命令来运行一个测试容器：

```shell
$ docker run hello-world
Unable to find image 'hello-world:latest' locally
latest: Pulling from library/hello-world
1b930d010525: Pull complete
Digest: sha256:c3b4ada4687bbaa170745b3e4dd8ac3f194ca95b2d0518b417fb47e5879d9b5f
Status: Downloaded newer image for hello-world:latest

Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
    (amd64)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker ID:
 https://hub.docker.com/

For more examples and ideas, visit:
 https://docs.docker.com/get-started/
```

---

## Kubernetes 基础

### 简介

[Kubernetes（简称 K8S）](https://kubernetes.io/) 的出现是容器化技术发展的必然结果，**容器化**是应用程序级别的虚拟化，运行单个内核上有多个独立的用户空间实例，这些实例就是容器；**容器**提供了将应用程序的代码、运行时、系统工具、系统库和配置打包到一个实例中的标准方法，而且容器是共享一个内核的；由于容器技术的兴起，导致大量的容器应用出现，所以就出现了一些用来支持应用程序容器化部署和组织的**容器编排**技术，一些流行的开源容器编排工具有 Docker Swarm、Kubernetes 等，但是在发展过程中 Kubernetes 现在已经成为了容器编排领域事实上的一个标准了。

---

### 架构

Kubernetes 项目依托着 Borg 项目的理论优势，确定了一个如下图所示的全局架构图：

- **全局架构**

![image-20230220145527666](https://raw.githubusercontent.com/MarcLan/pic/main/image-20230220145527666.png)

- **组件架构**

![image-20230220145755458](C:\Users\l84233197\AppData\Roaming\Typora\typora-user-images\image-20230220145755458.png)

---

### 组件

#### kube-apiserver

API Server 提供了资源对象的唯一操作入口，其它所有组件都必须通过它提供的 API 来操作资源数据。**只有 API Server 会与 etcd 进行通信，其它模块都必须通过 API Server 访问集群状态**。API Server 作为 Kubernetes 系统的入口，封装了核心对象的增删改查操作。API Server 以 RESTFul 接口方式提供给外部客户端和内部组件调用，API Server 再对相关的资源数据（`全量查询 + 变化监听`）进行操作，以达到实时完成相关的业务功能。以 API Server 为 Kubernetes 入口的设计主要有以下好处：

- 保证了集群状态访问的安全
- API Server 隔离了集群状态访问和后端存储实现，这样 API Server 状态访问的方式不会因为后端存储技术 Etcd 的改变而改变，让后端存储方式选择更加灵活，方便了整个架构的扩展

#### kube-contrller-manager

Controller Manager 用于实现 Kubernetes 集群故障检测和恢复的自动化工作。主要负责执行各种控制器：

- Replication Controller：主要是定期关联 Replication Controller (RC) 和 Pod，以保证集群中一个 RC (一种资源对象) 所关联的 Pod 副本数始终保持为与预设值一致。
- Node Controller：Kubelet 在启动时会通过 API Server 注册自身的节点信息，并定时向 API Server 汇报状态信息。API Server 在接收到信息后将信息更新到 Etcd 中。Node Controller 通过 API Server 实时获取 Node 的相关信息，实现管理和监控集群中的各个 Node 节点的相关控制功能。
- ResourceQuota Controller：资源配额管理控制器用于确保指定的资源对象在任何时候都不会超量占用系统上物理资源。
- Namespace Controller：用户通过 API Server 可以创建新的 Namespace 并保存在 Etcd 中，Namespace Controller 定时通过 API Server 读取这些 Namespace 信息来操作 Namespace。
- Service Account Controller：服务账号控制器主要在命名空间内管理 ServiceAccount，以保证名为 default 的 ServiceAccount 在每个命名空间中存在。
- Token Controller：令牌控制器作为 Controller Manager 的一部分，主要用作：监听 serviceAccount 的创建和删除动作以及监听 secret 的添加、删除动作。
- Service Controller：服务控制器主要用作监听 Service 的变化。
- Endpoint Controller：Endpoints 表示了一个 Service 对应的所有 Pod 副本的访问地址，而 Endpoints Controller 是负责生成和维护所有 Endpoints 对象的控制器。

#### kube-scheduler

Scheduler 是负责整个集群的资源调度的，主要的职责如下所示：

- 主要用于收集和分析当前 Kubernetes 集群中所有 Node 节点的资源 (包括内存、CPU 等) 负载情况，然后依据资源占用情况分发新建的 Pod 到 Kubernetes 集群中可用的节点
- 实时监测 Kubernetes 集群中未分发和已分发的所有运行的 Pod
- 实时监测 Node 节点信息，由于会频繁查找 Node 节点，所以 Scheduler 同时会缓存一份最新的信息在本地
- 在分发 Pod 到指定的 Node 节点后，会把 Pod 相关的 Binding 信息写回 API Server，以方便其它组件使用

#### kubelet

kubelet 是负责容器真正运行的核心组件，主要的职责如下所示：

- 负责 Node 节点上 Pod 的创建、修改、监控、删除等全生命周期的管理

- 定时上报本地 Node 的状态信息给 API Server

- kubelet 是 Master 和 Node 之间的桥梁，接收 API Server 分配给它的任务并执行

- kubelet 通过 API Server 间接与 Etcd 集群交互来读取集群配置信息

- kubelet 在 Node 上做的主要工作具体如下：

  - 设置容器的环境变量、给容器绑定 Volume、给容器绑定 Port、根据指定的 Pod 运行一个单一容器、给指定的 Pod 创建 Network 容器

  - 同步 Pod 的状态

  - 在容器中运行命令、杀死容器、删除 Pod 的所有容器

#### kube-proxy

kube-proxy 是为了解决外部网络能够访问集群中容器提供的应用服务而设计的，Proxy 运行在每个Node 上。

每创建一个 Service，kube-proxy 就会从 API Server 获取 Services 和 Endpoints 的配置信息，然后根据其配置信息在 Node 上启动一个 Proxy 的进程并监听相应的服务端口。

当接收到外部请求时，kube-proxy 会根据 Load Balancer 将请求分发到后端正确的容器处理。

kube-proxy 不但解决了同一宿主机相同服务端口冲突的问题，还提供了 Service 转发服务端口对外提供服务的能力。

kube-proxy 后端使用`随机、轮循`等负载均衡算法进行调度。

#### kubectl

Kubectl 是 Kubernetes 的集群管理命令行客户端工具集。通过 Kubectl 命令对 API Server 进行操作，API Server 响应并返回对应的命令结果，从而达到对 Kubernetes 集群的管理

#### Pod

Pod 是一组紧密关联的`容器集合`,它们共享 PID、IPC、Network 和 UTS namespace，是Kubernetes 调度的`基本单位`。

![image-20230220152752575](https://raw.githubusercontent.com/MarcLan/pic/main/image-20230220152752575.png)

在 Kubernetes 中，所有资源对象都使用资源清单（yaml或json）来定义，比如我们可以定义一个简单的 nginx 服务，它包含一个镜像为 nginx 的容器：(nginx-pod.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:  
  name: nginx  
  labels:    
    app: nginx
spec:  
  containers:  
  - name: nginx    
    image: nginx    
    ports:    
    - containerPort: 80
```

Pod 在 Kubernetes 集群中被创建的基本流程如下所示：

![image-20230220153310321](https://raw.githubusercontent.com/MarcLan/pic/main/image-20230220153310321.png)

- 用户通过 REST API 创建一个 Pod
- apiserver 将其写入 etcd
- scheduluer 检测到未绑定 Node 的 Pod，开始调度并更新 Pod 的 Node 绑定
- kubelet 检测到有新的 Pod 调度过来，通过 container runtime 运行该 Pod
- kubelet 通过 container runtime 取到 Pod 状态，并更新到 apiserver 中

### YAML

`YAML` 是专门用来写配置文件的语言，非常简洁和强大，远比 `JSON` 格式方便。`YAML`语言（发音 /ˈjæməl/）的设计目标，就是方便人类读写。它实质上是一种通用的数据串行化格式。

它的基本语法规则如下：

- 大小写敏感
- 使用缩进表示层级关系
- 缩进时不允许使用`Tab`键，只允许使用空格
- 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可
- `#` 表示注释，从这个字符一直到行尾，都会被解析器忽略

在 Kubernetes 中，我们只需要了解两种结构类型就行了：

- Lists（列表）
- Maps（字典）

#### Maps

 `Map` 是字典，就是一个 **key:value** 的键值对，`Maps` 可以让我们更加方便的去书写配置信息，例如：

```yaml
---
apiVersion: v1
kind: Pod
---
apiVersion: v1
kind: Pod
metadata:
  name: ydzs-site
  labels:
    app: web
```

#### Lists

数组

```yaml
args
  - Cat
  - Dog
  - Fish
```

```yaml
{
    "args": [ 'Cat', 'Dog', 'Fish' ]
}
```

---

### Pod

#### Why Pod

假设 Kubernetes 中调度的基本单元就是容器，对于一个非常简单的应用可以直接被调度直接使用，没有什么问题，但是往往还有很多应用程序是由多个进程组成的，可能会说把这些进程都打包到一个容器中去不就可以了吗？理论上是可以实现的，但是不要忘记了 Docker 管理的进程是 pid=1 的主进程，其他进程死掉了就会成为僵尸进程，没办法进行管理了，这种方式本身也不是容器推荐的运行方式，一个容器最好`只干一件事情`，所以在真实的环境中不会使用这种方式。

那么我们就把这个应用的进程进行拆分，拆分成一个一个的容器总可以了吧？但是不要忘记一个问题，拆分成一个一个的容器后，是不是就有可能出现一个应用下面的某个进程容器被调度到了不同的节点上呀？往往我们应用内部的进程与进程间通信（通过 IPC 或者共享本地文件之类）都是要求在本地进行的，也就是需要在同一个节点上运行。

所以我们需要一个更高级别的结构来将这些容器绑定在一起，并将他们作为一个基本的调度单元进行管理，这样就可以保证这些容器始终在同一个节点上面，这也就是 Pod 设计的初衷。

#### Pod 原理

在一个 Pod 下面运行几个关系非常密切的容器进程，这样一来这些进程本身又可以收到容器的管控，又具有几乎一致的运行环境，也就完美解决了上面提到的问题。

其实 Pod 也只是一个逻辑概念，真正起作用的还是 Linux 容器的 Namespace 和 Cgroup 这两个最基本的概念，Pod 被创建出来其实是一组共享了一些资源的容器而已。首先 Pod 里面的所有容器，都是共享的同一个 Network Namespace，但是涉及到文件系统的时候，默认情况下 Pod 里面的容器之间的文件系统是完全隔离的，但是我们可以通过声明来共享同一个 Volume。

对于共享同一个 Network Namespace 这个概念是不是比较熟悉，我们之前在 Docker 网络模式的章节中讲解了网络的 Container 模式，我们可以指定新创建的容器和一个已经存在的容器共享一个 Network Namespace，在运行容器的时候只需要指定 `--net=container:目标容器名` 这个参数就可以了，但是这种模式有一个明显的问题那就是容器的启动有先后顺序问题，那么 Pod 是怎么来处理这个问题的呢？那就是加入一个中间容器（没有什么架构是加一个中间件解决不了的？），这个容器叫做 Infra 容器，而且这个容器在 Pod 中永远都是第一个被创建的容器，这样是不是其他容器都加入到这个 Infra 容器就可以了，这样就完全实现了 Pod 中的所有容器都和 Infra 容器共享同一个 Network Namespace 了，如下图所示：

![image-20230220160604208](https://raw.githubusercontent.com/MarcLan/pic/main/image-20230220160604208.png)

从上面图中我们可以看出普通的容器加入到了 Infra 容器的 Network Namespace 中，所以这个 Pod 下面的所有容器就是共享同一个 Network Namespace 了，普通容器不会创建自己的网卡，配置自己的 IP，而是和 Infra 容器共享 IP、端口范围等，而且容器之间的进程可以通过 lo 网卡设备进行通信：

- 也就是容器之间是可以直接使用 `localhost` 进行通信的；
- 看到的网络设备信息都是和 Infra 容器完全一样的；
- 也就意味着同一个 Pod 下面的容器运行的多个进程不能绑定相同的端口；
- 而且 Pod 的生命周期只跟 Infra 容器一致，而与容器 A 和 B 无关。

默认情况下容器的文件系统是互相隔离的，要实现共享只需要在 Pod 的顶层声明一个 Volume，然后在需要共享这个 Volume 的容器中声明挂载即可。

![image-20230220160740060](https://raw.githubusercontent.com/MarcLan/pic/main/image-20230220160740060.png)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  volumes: 
  - name: varlog
    hostPath: 
      path: /var/log/counter
  containers:
  - name: count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log
    image: busybox
    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
```

#### 如何划分 Pod

前后端分离

![image-20230220161101148](https://raw.githubusercontent.com/MarcLan/pic/main/image-20230220161101148.png)

前后端都部署在一个容器中不利于扩容

![image-20230220161342805](https://raw.githubusercontent.com/MarcLan/pic/main/image-20230220161342805.png)

将多个容器部署到同一个 Pod 中的最主要参考就是应用可能由一个主进程和一个或多个的辅助进程组成，比如上面我们的日志收集的 Pod，需要其他的 sidecar 容器来支持日志的采集。所以当我们判断是否需要在 Pod 中使用多个容器的时候，我们可以按照如下的几个方式来判断：

- 这些容器是否一定需要一起运行，是否可以运行在不同的节点上
- 这些容器是一个整体还是独立的组件
- 这些容器一起进行扩缩容会影响应用吗

---

### Pod 的生命周期

生命周期包含： `Init Container`、`Pod Hook`、`健康检查` 

![image-20230220161601484](https://raw.githubusercontent.com/MarcLan/pic/main/image-20230220161601484.png)

#### Pod 的状态

通过 `kubectl explain pod.status` 命令来了解关于 Pod 状态

- 挂起（Pending）：Pod 信息已经提交给了集群，但是还没有被调度器调度到合适的节点或者 Pod 里的镜像正在下载
- 运行中（Running）：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态
- 成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启
- 失败（Failed）：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非`0`状态退出或者被系统终止
- 未知（Unknown）：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败导致的

#### 重启策略

我们可以通过配置`restartPolicy`字段来设置 Pod 中所有容器的重启策略，其可能值为`Always，OnFailure 和 Never`，默认值为 `Always`。restartPolicy 仅指通过 kubelet 在同一节点上重新启动容器。通过 kubelet 重新启动的退出容器将以指数增加延迟（10s，20s，40s…）重新启动，上限为 5 分钟，并在成功执行 10 分钟后重置。不同类型的的控制器可以控制 Pod 的重启策略：

- Job：适用于一次性任务如批量计算，任务结束后 Pod 会被此类控制器清除。Job 的重启策略只能是`"OnFailure"`或者`"Never"`。
- Replication Controller, ReplicaSet, or Deployment，此类控制器希望 Pod 一直运行下去，它们的重启策略只能是`"Always"`。
- DaemonSet：每个节点上启动一个 Pod，很明显此类控制器的重启策略也应该是`"Always"`。

#### 初始化容器

Pod 中最新启动的 `Init Container`，也就是我们平时常说的**初始化容器**。

#### Pod 健康检查

现在在 Pod 的整个生命周期中，能影响到 Pod 的就只剩下健康检查这一部分了。在 Kubernetes 集群当中，我们可以通过配置`liveness probe（存活探针`）和`readiness probe（可读性探针）`来影响容器的生命周期：

- kubelet 通过使用 `liveness probe` 来确定你的应用程序是否正在运行，通俗点将就是**是否还活着**。一般来说，如果你的程序一旦崩溃了， Kubernetes 就会立刻知道这个程序已经终止了，然后就会重启这个程序。而我们的 liveness probe 的目的就是来捕获到当前应用程序还没有终止，还没有崩溃，如果出现了这些情况，那么就重启处于该状态下的容器，使应用程序在存在 bug 的情况下依然能够继续运行下去。
- kubelet 使用 `readiness probe` 来确定容器是否已经就绪可以接收流量过来了。这个探针通俗点讲就是说**是否准备好了**，现在可以开始工作了。只有当 Pod 中的容器都处于就绪状态的时候 kubelet 才会认定该 Pod 处于就绪状态，因为一个 Pod 下面可能会有多个容器。当然 Pod 如果处于非就绪状态，那么我们就会将他从 Service 的 Endpoints 列表中移除出来，这样我们的流量就不会被路由到这个 Pod 里面来了。

和前面的钩子函数一样的，我们这两个探针的支持下面几种配置方式：

- exec：执行一段命令
- http：检测某个 http 请求
- tcpSocket：使用此配置，kubelet 将尝试在指定端口上打开容器的套接字。如果可以建立连接，容器被认为是健康的，如果不能就认为是失败的。实际上就是检查端口。

存活探针的使用方法，首先我们用 exec 执行命令的方式来检测容器的存活，如下：（liveness-exec.yaml）

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
```

- `periodSeconds`：表示让 kubelet 每隔5秒执行一次存活探针，也就是每5秒执行一次上面的`cat /tmp/healthy`命令，如果命令执行成功了，将返回0，那么 kubelet 就会认为当前这个容器是存活的，如果返回的是非0值，那么 kubelet 就会把该容器杀掉然后重启它。默认是10秒，最小1秒。
- `initialDelaySeconds`：表示在第一次执行探针的时候要等待5秒，这样能够确保我们的容器能够有足够的时间启动起来。大家可以想象下，如果你的第一次执行探针等候的时间太短，是不是很有可能容器还没正常启动起来，所以存活探针很可能始终都是失败的，这样就会无休止的重启下去了，对吧？

#### Pod 资源配置

了解下 CGroup 里面对于 CPU 资源的单位换算：

```shell
1 CPU =  1000 millicpu（1 Core = 1000m）

0.5 CPU = 500 millicpu （0.5 Core = 500m）
```

- `spec.containers[].resources.limits.cpu`：CPU 上限值，可以短暂超过，容器也不会被停止
- `spec.containers[].resources.requests.cpu`：CPU请求值，Kubernetes 调度算法里的依据值，可以超过

定义一个 Pod，给容器的配置如下的资源：（pod-resource-demo1.yaml）

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-demo1
spec:
  containers:
  - name: resource-demo1
    image: nginx
    ports:
    - containerPort: 80
    resources:
      requests:
        memory: 50Mi
        cpu: 50m
      limits:
        memory: 100Mi
        cpu: 100m
```

---

## 控制器

#### ReplicaSet

`ReplicaSet（RS）` 的主要作用就是维持一组 Pod 副本的运行，保证一定数量的 Pod 在集群中正常运行，ReplicaSet 控制器会持续监听它说控制的这些 Pod 的运行状态，在 Pod 发送故障数量减少或者增加时会触发调谐过程，始终保持副本数量一定。

```yaml
apiVersion: apps/v1
kind: ReplicaSet  
metadata:
  name:  nginx-rs
  namespace: default
spec:
  replicas: 3  # 期望的 Pod 副本数量，默认值为1
  selector:  # Label Selector，必须匹配 Pod 模板中的标签
    matchLabels:
      app: nginx
  template:  # Pod 模板
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```

#### Deployment

Deployment 就会用**滚动更新（Rolling Update）**的方式来升级现在的 Pod, 就是**每个 Deployment 就对应集群中的一次部署**

```yaml
apiVersion: apps/v1
kind: Deployment  
metadata:
  name:  nginx-deploy
  namespace: default
spec:
  replicas: 3  # 期望的 Pod 副本数量，默认值为1
  selector:  # Label Selector，必须匹配 Pod 模板中的标签
    matchLabels:
      app: nginx
  template:  # Pod 模板
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```

![image-20230220164013916](https://raw.githubusercontent.com/MarcLan/pic/main/image-20230220164013916.png)

#### StatefulSet

- `无状态服务（Stateless Service）`：该服务运行的实例不会在本地存储需要持久化的数据，并且多个实例对于同一个请求响应的结果是完全一致的。
- `有状态服务（Stateful Service）`：就和上面的概念是对立的了，该服务运行的实例需要在本地存储持久化数据，比如上面的 MySQL 数据库。

功能特性：

- 稳定的、唯一的网络标识符
- 稳定的、持久化的存储
- 有序的、优雅的部署和缩放
- 有序的、优雅的删除和终止
- 有序的、自动滚动更新

![image-20230220172156666](https://raw.githubusercontent.com/MarcLan/pic/main/image-20230220172156666.png)

#### DaemonSet

`Daemon`，就是用来部署守护进程的，`DaemonSet`用于在每个 Kubernetes 节点中将守护进程的副本作为后台进程运行

- 集群存储守护程序，如 glusterd、ceph 要部署在每个节点上以提供持久性存储；
- 节点监控守护进程，如 Prometheus 监控集群，可以在每个节点上运行一个 node-exporter 进程来收集监控节点的信息；
- 日志收集守护程序，如 fluentd 或 logstash，在每个节点上运行以收集容器的日志
- 节点网络插件，比如 flannel、calico，在每个节点上运行为 Pod 提供网络服务。

Pod 运行在哪个节点上是由 Kubernetes 的调度器策略来决定的，然而，由 DaemonSet 控制器创建的 Pod 实际上提前已经确定了在哪个节点上了（Pod创建时指定了`.spec.nodeName`）

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-ds
  namespace: default
spec:
  selector:
    matchLabels:
      k8s-app: nginx
  template:
    metadata:
      labels:
        k8s-app: nginx
    spec:
      containers:
      - image: nginx:1.7.9
        name: nginx
        ports:
        - name: http
          containerPort: 80
```

![image-20230220172651274](https://raw.githubusercontent.com/MarcLan/pic/main/image-20230220172651274.png)

集群中的 Pod 和 Node 是`一一`对应d的，而 DaemonSet 会管理全部机器上的 Pod 副本，负责对它们进行更新和删除。

那么，DaemonSet 控制器是如何保证每个 Node 上有且只有一个被管理的 Pod 呢？

- 首先控制器从 Etcd 获取到所有的 Node 列表，然后遍历所有的 Node。
- 根据资源对象定义是否有调度相关的配置，然后分别检查 Node 是否符合要求。
- 在可运行 Pod 的节点上检查是否已有对应的 Pod，如果没有，则在这个 Node 上创建该 Pod；如果有，并且数量大于 1，那就把多余的 Pod 从这个节点上删除；如果有且只有一个 Pod，那就说明是正常情况。

#### Job and Cronjob

批量数据处理和分析的需求，当然也会有按时间来进行调度的工作，在我们的 Kubernetes 集群中为我们提供了 `Job` 和 `CronJob` 两种资源对象来应对我们的这种需求。

`Job` 负责处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。而`CronJob` 则就是在 `Job` 上加上了时间调度。

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: job-demo
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: counter
        image: busybox
        command:
        - "bin/sh"
        - "-c"
        - "for i in 9 8 7 6 5 4 3 2 1; do echo $i; done"
```

`CronJob` 其实就是在 `Job` 的基础上加上了时间调度，我们可以在给定的时间点运行一个任务，也可以周期性地在给定时间点运行。

crontab 的格式为：`分 时 日 月 星期 要运行的命令` 。

- 第1列分钟0～59
- 第2列小时0～23）
- 第3列日1～31
- 第4列月1～12
- 第5列星期0～7（0和7表示星期天）
- 第6列要运行的命令

```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: cronjob-demo
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: hello
            image: busybox
            args:
            - "bin/sh"
            - "-c"
            - "for i in 9 8 7 6 5 4 3 2 1; do echo $i; done"
```

